# -*- coding: utf-8 -*-
"""
Fine-tuning BERT multi-label (ToLD-BR) - Vers√£o Reestruturada

Reimplementa√ß√£o modular usando princ√≠pios de separa√ß√£o de responsabilidades.

Suporta:
- BCEWithLogitsLoss padr√£o (baseline)
- BCEWithLogitsLoss com pos_weight customizado
- Focal Loss com alpha autom√°tico ou manual
- Facilmente extens√≠vel para outras loss functions
- Execu√ß√£o de m√∫ltiplas configura√ß√µes via arquivo JSON

Autor: Sistema Reestruturado
Data: 2024
"""

import os
import sys
import argparse
import logging
from typing import Optional

# Configurar ambiente antes dos imports principais
from src.config import setup_environment
setup_environment()

# Imports dos m√≥dulos
from src.config import ModelConfig, LossConfig, DATASET_PATH, SEED
from src.utils import LoggingSetup, SystemInfo, Timer, validate_requirements
from src.config_manager import BatchExecutor, create_simple_config_example
from src.data import DataPersistence, calculate_class_weights
from src.models import ModelFactory, ModelValidator
from src.training import TrainingManager
from src.metrics import DetailedMetricsAnalyzer, MetricsReporter
from src.visualization import VisualizationSuite

logger = logging.getLogger(__name__)

def create_argument_parser() -> argparse.ArgumentParser:
    """Cria parser de argumentos da linha de comando."""
    parser = argparse.ArgumentParser(
        description="""
Fine-tuning BERT multi-label (ToLD-BR) - Vers√£o Reestruturada

Reimplementa√ß√£o modular usando Hugging Face Transformers com separa√ß√£o
de responsabilidades para m√°xima flexibilidade e manutenibilidade.

Exemplos de uso:
  # Baseline (BCE padr√£o)
  python main.py --train --validate
  
  # Com pos_weight customizado
  python main.py --train --pos-weight "7.75,1.47,1.95,12.30,6.66,11.75"
  
  # Com Focal Loss
  python main.py --train --use-focal-loss
  
  # Executar m√∫ltiplas configura√ß√µes
  python main.py --config configurator.json
  
  # Criar configura√ß√£o de exemplo
  python main.py --create-example-config
        """,
        formatter_class=argparse.RawTextHelpFormatter
    )

    # A√ß√µes principais
    parser.add_argument('--train', action='store_true', 
                       help='Treina o modelo')
    parser.add_argument('--test', action='store_true', 
                       help='Testa o modelo salvo')
    parser.add_argument('--validate', action='store_true', 
                       help='Usa valida√ß√£o durante treino')
    
    # Configura√ß√µes do modelo
    parser.add_argument('--model-name', type=str, 
                       default='google-bert/bert-base-multilingual-cased',
                       help='Nome do modelo HuggingFace')
    parser.add_argument('--epochs', type=int, default=3,
                       help='N√∫mero de √©pocas de treinamento')
    parser.add_argument('--batch-size', type=int, default=8,
                       help='Tamanho do batch')
    parser.add_argument('--learning-rate', type=float, default=4e-5,
                       help='Taxa de aprendizado')
    parser.add_argument('--max-seq-length', type=int, default=128,
                       help='Comprimento m√°ximo das sequ√™ncias')
    
    # Configura√ß√µes de loss
    parser.add_argument('--pos-weight', type=str, 
                       help='Pesos por classe (ex: "7.75,1.47,...")')
    parser.add_argument('--use-focal-loss', action='store_true', 
                       help='Usar Focal Loss')
    parser.add_argument('--focal-gamma', type=float, default=2.0,
                       help='Par√¢metro gamma da Focal Loss')
    
    # Configura√ß√µes de arquivo
    parser.add_argument('--config', type=str, 
                       help='Arquivo JSON com configura√ß√µes m√∫ltiplas')
    parser.add_argument('--dataset', type=str, default=DATASET_PATH,
                       help='Caminho para dataset CSV')
    parser.add_argument('--output-dir', type=str, default='outputs_bert',
                       help='Diret√≥rio de sa√≠da')
    
    # Utilit√°rios
    parser.add_argument('--create-example-config', action='store_true',
                       help='Cria arquivo de configura√ß√£o de exemplo')
    parser.add_argument('--system-info', action='store_true',
                       help='Mostra informa√ß√µes do sistema')
    parser.add_argument('--log-level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='N√≠vel de logging')
    parser.add_argument('--log-file', type=str,
                       help='Arquivo para salvar logs')
    
    return parser

def setup_logging_from_args(args):
    """Configura logging baseado nos argumentos."""
    LoggingSetup.setup_logging(
        level=args.log_level,
        log_file=args.log_file
    )

def run_single_training(args) -> None:
    """Executa treinamento √∫nico baseado em argumentos."""
    logger.info("\nüöÄ MODO DE TREINAMENTO √öNICO")
    logger.info("="*50)
    
    # Criar configura√ß√µes
    model_config = ModelConfig(
        model_name=args.model_name,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        max_seq_length=args.max_seq_length,
        evaluate_during_training=args.validate,
        output_dir=args.output_dir,
        best_model_dir=os.path.join(args.output_dir, "best_model")  # Definir diret√≥rio do melhor modelo
    )
    
    loss_config = LossConfig(
        use_focal_loss=args.use_focal_loss,
        focal_gamma=args.focal_gamma
    )
    
    # Configurar pos_weight se fornecido
    if args.pos_weight:
        try:
            loss_config.pos_weight = [float(x.strip()) for x in args.pos_weight.split(',')]
            logger.info(f"‚öñÔ∏è Pos weights configurados: {loss_config.pos_weight}")
        except ValueError as e:
            logger.error(f"‚ùå Erro ao parsear pos_weight: {e}")
            sys.exit(1)
    
    # Validar configura√ß√µes
    loss_config.validate()
    
    # Carregar dados
    with Timer("Carregamento de dados"):
        train_df, val_df, test_df = DataPersistence.load_or_create_splits(args.dataset)
    
    # Criar modelo
    with Timer("Cria√ß√£o do modelo"):
        model, tokenizer = ModelFactory.create_model_and_tokenizer(model_config)
        ModelValidator.validate_model_config(model, 6)  # 6 classes
        ModelValidator.test_model_forward(model, tokenizer)
    
    # Calcular alpha weights se necess√°rio
    if loss_config.use_focal_loss:
        logger.info("üßÆ Calculando pesos alpha para Focal Loss...")
        loss_config.focal_alpha_weights = calculate_class_weights(train_df)
    
    # Executar treinamento
    with Timer("Treinamento"):
        training_manager = TrainingManager(model_config, loss_config)
        
        # Criar datasets
        from src.data import MultiLabelDataset
        train_dataset = MultiLabelDataset(
            train_df['text'].tolist(),
            train_df['labels'].tolist(),
            tokenizer,
            model_config.max_seq_length
        )
        
        val_dataset = None
        if model_config.evaluate_during_training:
            val_dataset = MultiLabelDataset(
                val_df['text'].tolist(),
                val_df['labels'].tolist(),
                tokenizer,
                model_config.max_seq_length
            )
        
        test_dataset = MultiLabelDataset(
            test_df['text'].tolist(),
            test_df['labels'].tolist(),
            tokenizer,
            model_config.max_seq_length
        )
        
        # Treinar
        trainer = training_manager.setup_trainer(model, tokenizer, train_dataset, val_dataset)
        train_result, training_history = training_manager.train()
    
    # Avaliar usando o MELHOR modelo (n√£o o √∫ltimo)
    with Timer("Avalia√ß√£o"):
        test_metrics, test_probs = training_manager.evaluate(test_dataset, load_best_model=True)
        
        # Log m√©tricas
        from src.metrics import MetricsReporter
        MetricsReporter.log_metrics_summary(test_metrics, "Resultados no Teste")
    
    # An√°lise detalhada
    with Timer("An√°lise detalhada"):
        import torch
        import numpy as np
        
        y_true = torch.stack([test_dataset[i]['labels'] for i in range(len(test_dataset))]).numpy()
        y_pred = (test_probs >= 0.5).astype(int)
        
        analyzer = DetailedMetricsAnalyzer()
        detailed_metrics = analyzer.calculate_detailed_metrics(y_true, y_pred, test_probs)
        
        # Gerar relat√≥rio
        report = MetricsReporter.generate_classification_report(y_true, y_pred)
        print(report)
        
        # Salvar relat√≥rio
        report_path = os.path.join(args.output_dir, "classification_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info(f"üìÑ Relat√≥rio salvo em: {report_path}")
    
    # Gerar visualiza√ß√µes
    with Timer("Gera√ß√£o de visualiza√ß√µes"):
        viz_suite = VisualizationSuite(os.path.join(args.output_dir, "visualizations"))
        optimal_thresholds = viz_suite.generate_all_plots(
            training_history, y_true, y_pred, test_probs, 
            detailed_metrics['per_class_metrics']
        )
    
    # NOVO: Copiar melhor modelo para diret√≥rio principal se existir
    best_model_dir = model_config.best_model_dir
    if os.path.exists(best_model_dir):
        logger.info(f"\nüèÜ Copiando melhor modelo para diret√≥rio principal: {args.output_dir}")
        import shutil
        
        # Listar arquivos do melhor modelo
        best_model_files = os.listdir(best_model_dir)
        
        # Copiar cada arquivo
        for filename in best_model_files:
            src = os.path.join(best_model_dir, filename)
            dst = os.path.join(args.output_dir, filename)
            if os.path.isfile(src):
                shutil.copy2(src, dst)
        
        # Log informa√ß√µes do melhor modelo se dispon√≠vel
        metadata_path = os.path.join(best_model_dir, "best_model_metadata.json")
        if os.path.exists(metadata_path):
            import json
            with open(metadata_path, 'r') as f:
                best_metadata = json.load(f)
            logger.info(f"üìä Melhor modelo salvo: step={best_metadata.get('best_checkpoint_step')}, "
                       f"{best_metadata.get('best_metric_name')}={best_metadata.get('best_metric_value'):.4f}")
    
    logger.info("\n‚úÖ Treinamento √∫nico conclu√≠do com sucesso!")

def run_batch_execution(args) -> None:
    """Executa m√∫ltiplas configura√ß√µes de um arquivo JSON."""
    logger.info("\nüöÄ MODO DE EXECU√á√ÉO EM LOTE")
    logger.info("="*50)
    
    if not os.path.exists(args.config):
        logger.error(f"‚ùå Arquivo de configura√ß√£o n√£o encontrado: {args.config}")
        sys.exit(1)
    
    with Timer(f"Execu√ß√£o em lote ({args.config})"):
        batch_executor = BatchExecutor(args.dataset)
        results = batch_executor.execute_config_file(args.config)
    
    # Resumo final
    successful = sum(1 for _, result in results if 'error' not in result)
    total = len(results)
    
    logger.info(f"\nüìä RESUMO FINAL:")
    logger.info(f"Total de inst√¢ncias: {total}")
    logger.info(f"Bem-sucedidas: {successful}")
    logger.info(f"Com erro: {total - successful}")
    
    if successful > 0:
        # Encontrar melhor modelo
        best_model = max(
            (result for _, result in results if 'error' not in result),
            key=lambda x: x.get('test_metrics', {}).get('test_macro_f1', 0)
        )
        best_f1 = best_model.get('test_metrics', {}).get('test_macro_f1', 0)
        best_id = best_model.get('instance_id', 'N/A')
        
        logger.info(f"üèÜ Melhor modelo: {best_id} (F1-Macro: {best_f1:.4f})")

def run_model_testing(args) -> None:
    """Executa teste de modelo salvo."""
    logger.info("\nüß™ MODO DE TESTE")
    logger.info("="*50)
    
    # Verificar primeiro se existe o melhor modelo
    best_model_dir = os.path.join(args.output_dir, "best_model")
    model_path = os.path.join(args.output_dir, "pytorch_model.bin")
    best_model_path = os.path.join(best_model_dir, "pytorch_model.bin")
    
    # Determinar qual modelo usar
    if os.path.exists(best_model_path):
        logger.info(f"üèÜ Melhor modelo encontrado em: {best_model_dir}")
        model_dir_to_use = best_model_dir
        
        # Carregar metadados do melhor modelo se dispon√≠vel
        metadata_path = os.path.join(best_model_dir, "best_model_metadata.json")
        if os.path.exists(metadata_path):
            import json
            with open(metadata_path, 'r') as f:
                best_metadata = json.load(f)
            logger.info(f"üìä Usando melhor modelo: step={best_metadata.get('best_checkpoint_step')}, "
                       f"{best_metadata.get('best_metric_name')}={best_metadata.get('best_metric_value'):.4f}")
    elif os.path.exists(model_path):
        logger.info(f"‚ö†Ô∏è Melhor modelo n√£o encontrado. Usando modelo do diret√≥rio principal: {args.output_dir}")
        model_dir_to_use = args.output_dir
    else:
        logger.error("‚ùå Nenhum modelo encontrado. Execute o treinamento primeiro com --train")
        sys.exit(1)
    
    # Carregar modelo
    from src.training import ModelCheckpointer
    
    try:
        model, tokenizer, metadata = ModelCheckpointer.load_checkpoint(model_dir_to_use)
        logger.info("‚úÖ Modelo carregado com sucesso")
        
        if metadata:
            logger.info(f"üìä Modelo salvo em: {metadata.get('saved_at', 'N/A')}")
            
            # Se h√° informa√ß√µes do melhor modelo nos metadados
            if 'best_model_info' in metadata:
                best_info = metadata['best_model_info']
                logger.info(f"üèÜ Informa√ß√µes do melhor modelo:")
                logger.info(f"   Step: {best_info.get('best_checkpoint_step')}")
                logger.info(f"   {best_info.get('best_metric_name')}: {best_info.get('best_metric_value'):.4f}")
        
        # Carregar dados de teste
        _, _, test_df = DataPersistence.load_or_create_splits(args.dataset)
        
        # Criar dataset de teste
        from src.data import MultiLabelDataset
        test_dataset = MultiLabelDataset(
            test_df['text'].tolist(),
            test_df['labels'].tolist(),
            tokenizer,
            args.max_seq_length
        )
        
        # Avaliar
        config = ModelConfig(output_dir=args.output_dir)
        training_manager = TrainingManager(config)
        training_manager.trainer = training_manager.setup_trainer(
            model, tokenizer, test_dataset
        )
        
        # Avaliar sem tentar recarregar o melhor modelo (j√° estamos usando ele)
        test_metrics, test_probs = training_manager.evaluate(test_dataset, load_best_model=False)
        MetricsReporter.log_metrics_summary(test_metrics, "Resultados do Teste")
        
        # An√°lise detalhada adicional
        import torch
        import numpy as np
        from src.metrics import DetailedMetricsAnalyzer
        
        y_true = torch.stack([test_dataset[i]['labels'] for i in range(len(test_dataset))]).numpy()
        y_pred = (test_probs >= 0.5).astype(int)
        
        analyzer = DetailedMetricsAnalyzer()
        detailed_metrics = analyzer.calculate_detailed_metrics(y_true, y_pred, test_probs)
        
        # Gerar e salvar relat√≥rio
        report = MetricsReporter.generate_classification_report(y_true, y_pred)
        print(report)
        
        report_path = os.path.join(args.output_dir, "test_classification_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info(f"üìÑ Relat√≥rio de teste salvo em: {report_path}")
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao testar modelo: {e}")
        sys.exit(1)

def main():
    """Fun√ß√£o principal."""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    # Configurar logging
    setup_logging_from_args(args)
    
    # Header
    logger.info("\n" + "="*80)
    logger.info("ü§ñ BERT MULTI-LABEL FINE-TUNING - VERS√ÉO REESTRUTURADA")
    logger.info("="*80)
    
    # Validar depend√™ncias
    if not validate_requirements():
        sys.exit(1)
    
    # Executar comando espec√≠fico
    try:
        if args.system_info:
            SystemInfo.log_system_info()
            return
        
        if args.create_example_config:
            config_path = create_simple_config_example()
            logger.info(f"‚úÖ Configura√ß√£o de exemplo criada: {config_path}")
            return
        
        # Log informa√ß√µes do sistema
        SystemInfo.log_system_info()
        
        # Modos de execu√ß√£o
        if args.config:
            run_batch_execution(args)
        elif args.train:
            run_single_training(args)
        elif args.test:
            run_model_testing(args)
        else:
            parser.print_help()
            logger.warning("‚ö†Ô∏è Nenhuma a√ß√£o especificada. Use --help para ver as op√ß√µes.")
    
    except KeyboardInterrupt:
        logger.info("\n‚èπÔ∏è Execu√ß√£o interrompida pelo usu√°rio")
        sys.exit(0)
    except Exception as e:
        logger.error(f"\n‚ùå Erro fatal: {e}")
        if args.log_level == "DEBUG":
            import traceback
            traceback.print_exc()
        sys.exit(1)
    
    logger.info("\nüéâ Execu√ß√£o conclu√≠da!")

if __name__ == "__main__":
    main()