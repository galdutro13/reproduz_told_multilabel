# -*- coding: utf-8 -*-
"""
Cria√ß√£o e configura√ß√£o de modelos BERT para classifica√ß√£o multi-label.
"""

import torch
from typing import Tuple
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    AutoConfig,
    PreTrainedModel,
    PreTrainedTokenizer
)
import logging

from src.config import ModelConfig, NUM_LABELS

logger = logging.getLogger(__name__)

class ModelFactory:
    """Factory para cria√ß√£o de modelos BERT."""
    
    @staticmethod
    def create_model_and_tokenizer(config: ModelConfig) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
        """
        Cria modelo BERT e tokenizer para classifica√ß√£o multi-label.
        
        Args:
            config: Configura√ß√£o do modelo
            
        Returns:
            Tuple[PreTrainedModel, PreTrainedTokenizer]: Modelo e tokenizer
        """
        logger.info(f"ü§ñ Criando modelo: {config.model_name}")
        
        # Configurar modelo
        model_config = AutoConfig.from_pretrained(
            config.model_name,
            num_labels=NUM_LABELS,
            problem_type="multi_label_classification",
            cache_dir=config.cache_dir
        )
        
        # Criar modelo
        model = AutoModelForSequenceClassification.from_pretrained(
            config.model_name,
            config=model_config,
            cache_dir=config.cache_dir
        )
        
        # Criar tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            config.model_name,
            do_lower_case=config.do_lower_case,
            cache_dir=config.cache_dir
        )
        
        # Otimizar modelo se poss√≠vel
        model = ModelOptimizer.optimize_model(model)
        
        logger.info(f"‚úÖ Modelo criado com {model.num_parameters():,} par√¢metros")
        
        return model, tokenizer

class ModelOptimizer:
    """Classe para otimiza√ß√µes do modelo."""
    
    @staticmethod
    def optimize_model(model: PreTrainedModel) -> PreTrainedModel:
        """
        Aplica otimiza√ß√µes ao modelo.
        
        Args:
            model: Modelo a ser otimizado
            
        Returns:
            PreTrainedModel: Modelo otimizado
        """
        # Tentar torch.compile se dispon√≠vel (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                optimized_model = torch.compile(model, backend='ipex')
                logger.info("üõ†Ô∏è torch.compile ativado com backend ipex")
                return optimized_model
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è torch.compile falhou: {e}")
                
                # Tentar backend padr√£o
                try:
                    optimized_model = torch.compile(model)
                    logger.info("üõ†Ô∏è torch.compile ativado com backend padr√£o")
                    return optimized_model
                except Exception as e2:
                    logger.warning(f"‚ö†Ô∏è torch.compile com backend padr√£o falhou: {e2}")
        
        logger.info("üìù Usando modelo sem otimiza√ß√µes torch.compile")
        return model
    
    @staticmethod
    def get_model_info(model: PreTrainedModel) -> dict:
        """
        Retorna informa√ß√µes sobre o modelo.
        
        Args:
            model: Modelo para analisar
            
        Returns:
            dict: Informa√ß√µes do modelo
        """
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': (total_params * 4) / (1024 * 1024),  # Assumindo float32
            'config': model.config.__dict__ if hasattr(model, 'config') else {}
        }
    
    @staticmethod
    def print_model_summary(model: PreTrainedModel, model_name: str = "Model"):
        """
        Imprime resumo do modelo.
        
        Args:
            model: Modelo para analisar
            model_name: Nome do modelo para display
        """
        info = ModelOptimizer.get_model_info(model)
        
        logger.info(f"\nüìä Resumo do {model_name}:")
        logger.info(f"  Total de par√¢metros: {info['total_parameters']:,}")
        logger.info(f"  Par√¢metros trein√°veis: {info['trainable_parameters']:,}")
        logger.info(f"  Tamanho estimado: {info['model_size_mb']:.1f} MB")
        
        if info['config']:
            logger.info(f"  Configura√ß√£o:")
            for key, value in info['config'].items():
                if key in ['hidden_size', 'num_hidden_layers', 'num_attention_heads', 'vocab_size']:
                    logger.info(f"    {key}: {value}")

class ModelValidator:
    """Validador para verificar consist√™ncia do modelo."""
    
    @staticmethod
    def validate_model_config(model: PreTrainedModel, expected_num_labels: int):
        """
        Valida se o modelo est√° configurado corretamente.
        
        Args:
            model: Modelo para validar
            expected_num_labels: N√∫mero esperado de labels
            
        Raises:
            ValueError: Se a configura√ß√£o estiver incorreta
        """
        if not hasattr(model, 'config'):
            logger.warning("‚ö†Ô∏è Modelo n√£o possui atributo 'config'")
            return
        
        config = model.config
        
        # Verificar n√∫mero de labels
        if hasattr(config, 'num_labels') and config.num_labels != expected_num_labels:
            raise ValueError(
                f"Modelo configurado para {config.num_labels} labels, "
                f"mas esperado {expected_num_labels}"
            )
        
        # Verificar tipo de problema
        if hasattr(config, 'problem_type') and config.problem_type != "multi_label_classification":
            logger.warning(
                f"‚ö†Ô∏è Modelo configurado para '{config.problem_type}', "
                f"mas esperado 'multi_label_classification'"
            )
        
        logger.info("‚úÖ Configura√ß√£o do modelo validada")
    
    @staticmethod
    def test_model_forward(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, 
                          sample_text: str = "Texto de teste"):
        """
        Testa forward pass do modelo.
        
        Args:
            model: Modelo para testar
            tokenizer: Tokenizer
            sample_text: Texto de exemplo para teste
        """
        try:
            # Tokenizar texto de exemplo
            inputs = tokenizer(
                sample_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=128
            )
            
            # Forward pass
            model.eval()
            with torch.no_grad():
                outputs = model(**inputs)
            
            # Verificar shape dos logits
            expected_shape = (1, NUM_LABELS)
            actual_shape = outputs.logits.shape
            
            if actual_shape != expected_shape:
                raise ValueError(
                    f"Shape dos logits incorreto: {actual_shape}, esperado {expected_shape}"
                )
            
            logger.info("‚úÖ Teste de forward pass bem-sucedido")
            logger.info(f"   Input shape: {inputs['input_ids'].shape}")
            logger.info(f"   Output shape: {outputs.logits.shape}")
            
        except Exception as e:
            logger.error(f"‚ùå Erro no teste de forward pass: {e}")
            raise