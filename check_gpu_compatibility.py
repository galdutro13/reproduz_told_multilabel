#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para verificar compatibilidade e configura√ß√£o de GPU para o projeto.
Execute antes de iniciar o treinamento com GPU.
"""

import torch
import sys
import os
from typing import Dict, Any

def check_cuda_installation() -> Dict[str, Any]:
    """Verifica instala√ß√£o do CUDA."""
    info = {
        "cuda_available": torch.cuda.is_available(),
        "pytorch_version": torch.__version__,
        "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
        "cudnn_version": torch.backends.cudnn.version() if torch.cuda.is_available() else None,
    }
    return info

def check_gpu_devices() -> Dict[str, Any]:
    """Verifica dispositivos GPU dispon√≠veis."""
    if not torch.cuda.is_available():
        return {"gpu_count": 0, "devices": []}
    
    gpu_count = torch.cuda.device_count()
    devices = []
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        device_info = {
            "id": i,
            "name": props.name,
            "memory_total_gb": props.total_memory / 1e9,
            "memory_allocated_gb": torch.cuda.memory_allocated(i) / 1e9,
            "memory_cached_gb": torch.cuda.memory_reserved(i) / 1e9,
            "compute_capability": f"{props.major}.{props.minor}",
            "multiprocessor_count": props.multi_processor_count,
        }
        devices.append(device_info)
    
    return {"gpu_count": gpu_count, "devices": devices}

def test_gpu_performance():
    """Testa performance b√°sica da GPU."""
    if not torch.cuda.is_available():
        print("‚ùå CUDA n√£o dispon√≠vel - n√£o √© poss√≠vel testar GPU")
        return
    
    print("üß™ Testando performance da GPU...")
    
    # Teste de cria√ß√£o de tensor
    device = torch.device("cuda:0")
    try:
        # Criar tensores de teste
        a = torch.randn(1000, 1000, device=device)
        b = torch.randn(1000, 1000, device=device)
        
        # Teste de multiplica√ß√£o de matrizes
        import time
        start_time = time.time()
        for _ in range(100):
            c = torch.matmul(a, b)
        torch.cuda.synchronize()  # Aguardar conclus√£o
        gpu_time = time.time() - start_time
        
        # Teste na CPU para compara√ß√£o
        a_cpu = a.cpu()
        b_cpu = b.cpu()
        start_time = time.time()
        for _ in range(100):
            c_cpu = torch.matmul(a_cpu, b_cpu)
        cpu_time = time.time() - start_time
        
        speedup = cpu_time / gpu_time
        
        print(f"‚úÖ Teste de performance conclu√≠do:")
        print(f"   GPU: {gpu_time:.4f}s")
        print(f"   CPU: {cpu_time:.4f}s")
        print(f"   Acelera√ß√£o: {speedup:.2f}x")
        
        if speedup > 1.5:
            print("üöÄ GPU est√° funcionando bem!")
        else:
            print("‚ö†Ô∏è GPU pode n√£o estar otimizada")
            
    except Exception as e:
        print(f"‚ùå Erro no teste de GPU: {e}")

def check_memory_requirements():
    """Verifica se h√° mem√≥ria suficiente para treinamento."""
    if not torch.cuda.is_available():
        return
    
    # Estimativas conservadoras para BERT
    model_sizes = {
        "bert-base": 0.5,  # GB
        "bert-large": 1.3,  # GB
    }
    
    batch_memory_per_sample = {
        128: 0.15,  # GB por amostra para seq_len=128
        256: 0.25,  # GB por amostra para seq_len=256
        512: 0.45,  # GB por amostra para seq_len=512
    }
    
    gpu_info = check_gpu_devices()
    
    for device in gpu_info["devices"]:
        available_memory = device["memory_total_gb"] - device["memory_allocated_gb"]
        
        print(f"\nüìä GPU {device['id']} - {device['name']}:")
        print(f"   Mem√≥ria total: {device['memory_total_gb']:.1f} GB")
        print(f"   Mem√≥ria dispon√≠vel: {available_memory:.1f} GB")
        
        # Calcular batch sizes recomendados
        for seq_len, memory_per_sample in batch_memory_per_sample.items():
            model_memory = model_sizes["bert-base"]
            overhead = 1.0  # GB de overhead
            
            usable_memory = available_memory - model_memory - overhead
            if usable_memory > 0:
                max_batch_size = int(usable_memory / memory_per_sample)
                print(f"   Batch size recomendado (seq_len={seq_len}): {max_batch_size}")
            else:
                print(f"   ‚ö†Ô∏è Mem√≥ria insuficiente para seq_len={seq_len}")

def check_environment_variables():
    """Verifica vari√°veis de ambiente importantes."""
    important_vars = [
        "CUDA_VISIBLE_DEVICES",
        "CUDA_LAUNCH_BLOCKING",
        "PYTORCH_CUDA_ALLOC_CONF",
    ]
    
    print("\nüîß Vari√°veis de ambiente CUDA:")
    for var in important_vars:
        value = os.environ.get(var, "N√£o definida")
        print(f"   {var}: {value}")

def suggest_optimizations():
    """Sugere otimiza√ß√µes baseadas no hardware detectado."""
    gpu_info = check_gpu_devices()
    
    if gpu_info["gpu_count"] == 0:
        print("\nüí° Sugest√µes:")
        print("   - Instale drivers NVIDIA e CUDA")
        print("   - Verifique se PyTorch foi instalado com suporte CUDA")
        return
    
    print("\nüí° Sugest√µes de otimiza√ß√£o:")
    
    for device in gpu_info["devices"]:
        memory_gb = device["memory_total_gb"]
        
        if memory_gb >= 24:
            print(f"   GPU {device['id']}: GPU de alta capacidade detectada")
            print("     - Use batch_size=32-64")
            print("     - Ative FP16 para maior throughput")
            print("     - Considere usar gradient_accumulation_steps=1")
        elif memory_gb >= 12:
            print(f"   GPU {device['id']}: GPU de capacidade m√©dia detectada")
            print("     - Use batch_size=16-32")
            print("     - Ative FP16 para economia de mem√≥ria")
            print("     - Use gradient_accumulation_steps=2 se necess√°rio")
        elif memory_gb >= 8:
            print(f"   GPU {device['id']}: GPU de capacidade b√°sica detectada")
            print("     - Use batch_size=8-16")
            print("     - Ative FP16 obrigatoriamente")
            print("     - Use gradient_accumulation_steps=4")
            print("     - Considere usar gradient_checkpointing")
        else:
            print(f"   GPU {device['id']}: Mem√≥ria limitada")
            print("     - Use batch_size=4-8")
            print("     - Ative todas as otimiza√ß√µes de mem√≥ria")
            print("     - Considere usar CPU em vez de GPU")

def main():
    """Fun√ß√£o principal de verifica√ß√£o."""
    print("üîç VERIFICA√á√ÉO DE COMPATIBILIDADE GPU")
    print("=" * 50)
    
    # Verificar CUDA
    cuda_info = check_cuda_installation()
    print("\nüéØ Instala√ß√£o CUDA:")
    print(f"   CUDA dispon√≠vel: {'‚úÖ Sim' if cuda_info['cuda_available'] else '‚ùå N√£o'}")
    print(f"   PyTorch vers√£o: {cuda_info['pytorch_version']}")
    
    if cuda_info['cuda_available']:
        print(f"   CUDA vers√£o: {cuda_info['cuda_version']}")
        print(f"   cuDNN vers√£o: {cuda_info['cudnn_version']}")
    else:
        print("\n‚ùå CUDA n√£o est√° dispon√≠vel!")
        print("Instru√ß√µes para instalar:")
        print("1. Instale drivers NVIDIA apropriados")
        print("2. Instale CUDA Toolkit")
        print("3. Reinstale PyTorch com suporte CUDA:")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        return
    
    # Verificar GPUs
    gpu_info = check_gpu_devices()
    print(f"\nüéÆ GPUs dispon√≠veis: {gpu_info['gpu_count']}")
    
    for device in gpu_info["devices"]:
        print(f"\n   GPU {device['id']}: {device['name']}")
        print(f"     Mem√≥ria: {device['memory_total_gb']:.1f} GB")
        print(f"     Compute Capability: {device['compute_capability']}")
        print(f"     Multiprocessors: {device['multiprocessor_count']}")
    
    # Testar performance
    test_gpu_performance()
    
    # Verificar requisitos de mem√≥ria
    check_memory_requirements()
    
    # Verificar vari√°veis de ambiente
    check_environment_variables()
    
    # Sugerir otimiza√ß√µes
    suggest_optimizations()
    
    print(f"\n‚úÖ Verifica√ß√£o conclu√≠da!")
    print("Execute 'python main.py --train --validate' para iniciar o treinamento com GPU")

if __name__ == "__main__":
    main()