import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import warnings
from pathlib import Path
from scipy import stats

warnings.filterwarnings('ignore')

# ============================================================================
# CLASSE PRINCIPAL DE AN√ÅLISE
# ============================================================================

class SimpleMultiModelAnalyzer:
    """Vers√£o simplificada para execu√ß√£o direta."""
    
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.labels = ["homophobia", "obscene", "insult", "racism", "misogyny", "xenophobia"]
        self.models = ["BERT", "Bertimbau", "BertAbaporu"]
        self.loss_types = ["Default", "Focal", "PosWeight"]
        
        # Mapeamento ID -> (Modelo, Loss)
        self.model_mapping = {
            "I001": ("BERT", "Default"),
            "I002": ("BERT", "Focal"),
            "I003": ("BERT", "PosWeight"),
            "I004": ("Bertimbau", "Default"),
            "I005": ("Bertimbau", "Focal"),
            "I006": ("Bertimbau", "PosWeight"),
            "I007": ("BertAbaporu", "Default"),
            "I008": ("BertAbaporu", "Focal"),
            "I009": ("BertAbaporu", "PosWeight")
        }
        
        self.results_data = {}
        self.comparison_df = None
    
    def load_all_results(self):
        """Carrega todos os resultados dos modelos."""
        print("üìÇ CARREGANDO RESULTADOS DOS MODELOS")
        print("="*50)
        
        for model_id, (model_name, loss_type) in self.model_mapping.items():
            results_path = self.base_path / model_id / "results.json"
            
            if results_path.exists():
                try:
                    with open(results_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    self.results_data[model_id] = {
                        'model_name': model_name,
                        'loss_type': loss_type,
                        'data': data
                    }
                    print(f"   ‚úÖ {model_name:<12} {loss_type:<10} - OK")
                    
                except Exception as e:
                    print(f"   ‚ùå {model_name:<12} {loss_type:<10} - ERRO: {e}")
            else:
                print(f"   ‚ö†Ô∏è  {model_name:<12} {loss_type:<10} - N√ÉO ENCONTRADO")
        
        print(f"\nüìä TOTAL CARREGADO: {len(self.results_data)}/9 modelos\n")
        
        if len(self.results_data) == 0:
            print("‚ùå NENHUM RESULTADO ENCONTRADO!")
            print("Verifique se os diret√≥rios I001-I009 existem e cont√™m results.json")
            return False
        
        return True
    
    def create_comparison_table(self):
        """Cria tabela comparativa principal."""
        print("üìã CRIANDO TABELA COMPARATIVA")
        print("="*50)
        
        comparison_data = []
        
        for model_id, info in self.results_data.items():
            model_name = info['model_name']
            loss_type = info['loss_type']
            data = info['data']
            
            # M√©tricas principais
            test_metrics = data.get('test_metrics', {})
            
            record = {
                'ID': model_id,
                'Modelo': model_name,
                'Loss': loss_type,
                'Macro_F1': test_metrics.get('test_macro_f1', 0.0),
                'Avg_Precision': test_metrics.get('test_avg_precision', 0.0),
                'Hamming_Loss': test_metrics.get('test_hamming_loss', 1.0),
                'Micro_F1': test_metrics.get('test_micro_f1', 0.0),
                'Tempo_Treino': data.get('training_time', 0.0)
            }
            
            # F1 por classe
            per_class = data.get('detailed_metrics', {}).get('per_class_metrics', {})
            for label in self.labels:
                if label in per_class:
                    record[f'F1_{label}'] = per_class[label].get('f1', 0.0)
                else:
                    record[f'F1_{label}'] = 0.0
            
            comparison_data.append(record)
        
        self.comparison_df = pd.DataFrame(comparison_data)
        self.comparison_df = self.comparison_df.sort_values('Macro_F1', ascending=False)
        
        print("‚úÖ Tabela criada com sucesso!")
        print(f"üìä {len(self.comparison_df)} modelos na compara√ß√£o\n")
        
        return self.comparison_df
    
    def show_rankings(self):
        """Mostra rankings principais."""
        if self.comparison_df is None:
            self.create_comparison_table()
        
        print("üèÜ RANKING GERAL (TOP 5)")
        print("="*50)
        
        top_5 = self.comparison_df.head()
        for i, (_, row) in enumerate(top_5.iterrows(), 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}."
            print(f"{emoji} {row['Modelo']:<12} {row['Loss']:<10} | "
                  f"F1: {row['Macro_F1']:.4f} | AP: {row['Avg_Precision']:.4f}")
        
        print("\nüìä PERFORMANCE POR MODELO")
        print("="*50)
        
        model_stats = self.comparison_df.groupby('Modelo')['Macro_F1'].agg(['mean', 'max', 'std']).round(4)
        model_stats = model_stats.sort_values('mean', ascending=False)
        
        for model, stats in model_stats.iterrows():
            print(f"{model:<12}: M√©dia={stats['mean']:.4f} | "
                  f"Melhor={stats['max']:.4f} | Desvio=¬±{stats['std']:.3f}")
        
        print("\n‚öñÔ∏è  PERFORMANCE POR LOSS")
        print("="*50)
        
        loss_stats = self.comparison_df.groupby('Loss')['Macro_F1'].agg(['mean', 'max', 'std']).round(4)
        loss_stats = loss_stats.sort_values('mean', ascending=False)
        
        for loss, stats in loss_stats.iterrows():
            print(f"{loss:<10}: M√©dia={stats['mean']:.4f} | "
                  f"Melhor={stats['max']:.4f} | Desvio=¬±{stats['std']:.3f}")
    
    def analyze_problematic_classes(self):
        """Analisa classes com baixa performance."""
        print("\nüéØ AN√ÅLISE DE CLASSES PROBLEM√ÅTICAS")
        print("="*50)
        
        # Calcular F1 m√©dio por classe
        class_performance = {}
        for label in self.labels:
            col_name = f'F1_{label}'
            if col_name in self.comparison_df.columns:
                avg_f1 = self.comparison_df[col_name].mean()
                max_f1 = self.comparison_df[col_name].max()
                best_model_idx = self.comparison_df[col_name].idxmax()
                best_model = self.comparison_df.loc[best_model_idx]
                
                class_performance[label] = {
                    'avg_f1': avg_f1,
                    'max_f1': max_f1,
                    'best_model': best_model['Modelo'],
                    'best_loss': best_model['Loss']
                }
        
        # Ordenar por performance (pior para melhor)
        sorted_classes = sorted(class_performance.items(), key=lambda x: x[1]['avg_f1'])
        
        print("Classes ordenadas por dificuldade (mais dif√≠cil primeiro):\n")
        
        for label, stats in sorted_classes:
            status = "üî¥" if stats['avg_f1'] < 0.2 else "üü°" if stats['avg_f1'] < 0.4 else "üü¢"
            print(f"{status} {label:<12}: M√©dia={stats['avg_f1']:.3f} | "
                  f"Melhor={stats['max_f1']:.3f} | "
                  f"Top: {stats['best_model']} ({stats['best_loss']})")
        
        # Identificar classes cr√≠ticas
        critical_classes = [label for label, stats in class_performance.items() 
                            if stats['avg_f1'] < 0.3]
        
        if critical_classes:
            print(f"\nüö® CLASSES CR√çTICAS (F1 < 0.3): {', '.join(critical_classes)}")
            print("üí° Recomenda√ß√µes:")
            print("   - Coletar mais dados para essas classes")
            print("   - Usar t√©cnicas de data augmentation")
            print("   - Considerar class weights mais altos")
    
    def create_separated_plots(self, output_dir: str = "analysis_output"):
        """Cria os 4 gr√°ficos separados."""
        print(f"\nüî• GERANDO GR√ÅFICOS SEPARADOS")
        print("="*50)

        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Plot 1: Heatmap principal
        plt.figure(figsize=(8, 6))
        pivot_data = self.comparison_df.pivot(index='Modelo', columns='Loss', values='Macro_F1')
        sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', 
                    cbar_kws={'label': 'Macro F1-Score'})
        plt.title('Performance: Modelo √ó Loss Function')
        plt.ylabel('Modelo Base')
        plt.xlabel('Tipo de Loss')
        heatmap_path = output_path / "heatmap_model_loss.png"
        plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')
        plt.close() # Fecha a figura para n√£o exibir ou sobrepor no pr√≥ximo plot
        print(f"üìä Heatmap Model x Loss salvo em: {heatmap_path}")
        
        # Plot 2: Ranking geral
        plt.figure(figsize=(8, 6))
        top_models = self.comparison_df.head(9)
        y_pos = range(len(top_models))
        bars = plt.barh(y_pos, top_models['Macro_F1'])
        plt.yticks(y_pos, [f"{row['Modelo']}\n({row['Loss']})" 
                           for _, row in top_models.iterrows()])
        plt.xlabel('Macro F1-Score')
        plt.title('Ranking Geral (Todos os Modelos)')
        plt.grid(True, alpha=0.3)
        colors = {'BERT': '#FF6B6B', 'Bertimbau': '#4ECDC4', 'BertAbaporu': '#45B7D1'}
        for i, (_, row) in enumerate(top_models.iterrows()):
            bars[i].set_color(colors.get(row['Modelo'], '#95A5A6'))
        ranking_path = output_path / "ranking_geral.png"
        plt.savefig(ranking_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"üìä Ranking Geral salvo em: {ranking_path}")

        # Plot 3: Performance por classe (classes problem√°ticas)
        plt.figure(figsize=(8, 6))
        problem_classes = ['racism', 'xenophobia', 'misogyny'] # Exemplo, ajuste conforme sua an√°lise
        class_data = []
        class_labels = []
        
        for label in problem_classes:
            col_name = f'F1_{label}'
            if col_name in self.comparison_df.columns:
                class_data.append(self.comparison_df[col_name].values)
                class_labels.append(label)
        
        if class_data:
            plt.boxplot(class_data, labels=class_labels)
            plt.ylabel('F1-Score')
            plt.title('Distribui√ß√£o: Classes Problem√°ticas')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
            problematic_classes_path = output_path / "f1_problematic_classes.png"
            plt.savefig(problematic_classes_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"üìä F1 Classes Problem√°ticas salvo em: {problematic_classes_path}")
        else:
            print("‚ö†Ô∏è N√£o foi poss√≠vel gerar o gr√°fico de classes problem√°ticas. Verifique as colunas.")

        # Plot 4: Loss function effectiveness
        plt.figure(figsize=(8, 6))
        loss_means = self.comparison_df.groupby('Loss')['Macro_F1'].mean()
        loss_stds = self.comparison_df.groupby('Loss')['Macro_F1'].std()
        
        x_pos = range(len(loss_means))
        plt.bar(x_pos, loss_means.values, yerr=loss_stds.values, 
                capsize=5, alpha=0.7, color=['#FF9999', '#66B2FF', '#99FF99'])
        plt.xticks(x_pos, loss_means.index)
        plt.ylabel('Macro F1-Score')
        plt.title('Efetividade por Loss Function')
        plt.grid(True, alpha=0.3)
        loss_effectiveness_path = output_path / "effectiveness_loss_function.png"
        plt.savefig(loss_effectiveness_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"üìä Efetividade da Loss Function salvo em: {loss_effectiveness_path}")
        
        print("\n‚úÖ Todos os gr√°ficos separados foram gerados com sucesso!")
    
    def generate_final_report(self, output_dir: str = "analysis_output"):
        """Gera relat√≥rio final."""
        print(f"\nüìÑ GERANDO RELAT√ìRIO FINAL")
        print("="*50)
        
        # Criar diret√≥rio
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Salvar tabela completa
        csv_path = output_path / "comparison_complete.csv"
        self.comparison_df.to_csv(csv_path, index=False)
        print(f"üíæ Tabela completa salva: {csv_path}")
        
        # Relat√≥rio em texto
        report_path = output_path / "final_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("RELAT√ìRIO FINAL - AN√ÅLISE MULTI-MODELO BERT\n")
            f.write("="*60 + "\n\n")
            
            # Resumo executivo
            best_model = self.comparison_df.iloc[0]
            f.write("RESUMO EXECUTIVO:\n")
            f.write(f"Melhor modelo geral: {best_model['Modelo']} com {best_model['Loss']}\n")
            f.write(f"Macro F1-Score: {best_model['Macro_F1']:.4f}\n")
            f.write(f"Average Precision: {best_model['Avg_Precision']:.4f}\n\n")
            
            # Top 5
            f.write("TOP 5 MODELOS:\n")
            f.write("-" * 40 + "\n")
            for i, (_, row) in enumerate(self.comparison_df.head().iterrows(), 1):
                f.write(f"{i}. {row['Modelo']} ({row['Loss']}) - F1: {row['Macro_F1']:.4f}\n")
            
            f.write("\n")
            
            # Performance por modelo
            f.write("PERFORMANCE POR MODELO BASE:\n")
            f.write("-" * 40 + "\n")
            model_stats = self.comparison_df.groupby('Modelo')['Macro_F1'].agg(['mean', 'max']).round(4)
            for model, stats in model_stats.sort_values('mean', ascending=False).iterrows():
                f.write(f"{model}: M√©dia={stats['mean']:.4f} | Melhor={stats['max']:.4f}\n")
            
            f.write("\n")
            
            # Performance por loss
            f.write("PERFORMANCE POR LOSS FUNCTION:\n")
            f.write("-" * 40 + "\n")
            loss_stats = self.comparison_df.groupby('Loss')['Macro_F1'].agg(['mean', 'max']).round(4)
            for loss, stats in loss_stats.sort_values('mean', ascending=False).iterrows():
                f.write(f"{loss}: M√©dia={stats['mean']:.4f} | Melhor={stats['max']:.4f}\n")
            
            # Recomenda√ß√µes
            f.write("\nRECOMENDA√á√ïES:\n")
            f.write("-" * 40 + "\n")
            f.write(f"1. Use {best_model['Modelo']} com {best_model['Loss']} para m√°xima performance\n")
            
            # Melhor por categoria
            best_by_model = self.comparison_df.groupby('Modelo')['Macro_F1'].idxmax()
            for model in self.models:
                if model in best_by_model.index:
                    idx = best_by_model[model]
                    row = self.comparison_df.loc[idx]
                    f.write(f"2. Para {model}: use {row['Loss']} (F1: {row['Macro_F1']:.4f})\n")
            
            f.write("3. Foque em melhorar classes problem√°ticas (racism, xenophobia)\n")
            f.write("4. Implemente early stopping para evitar overfitting\n")
        
        print(f"üìÑ Relat√≥rio salvo: {report_path}")
        
        # Criar README
        readme_path = output_path / "README.txt"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write("COMO LER OS RESULTADOS\n")
            f.write("="*30 + "\n\n")
            f.write("1. final_report.txt - Resumo executivo com principais conclus√µes\n")
            f.write("2. comparison_complete.csv - Dados completos para an√°lise posterior\n")
            f.write("3. heatmap_model_loss.png - Visualiza√ß√£o comparativa de Modelo x Fun√ß√£o de Perda\n")
            f.write("4. ranking_geral.png - Ranking de todos os modelos por Macro F1-Score\n")
            f.write("5. f1_problematic_classes.png - Desempenho F1-Score em classes problem√°ticas\n")
            f.write("6. effectiveness_loss_function.png - Efetividade das Fun√ß√µes de Perda\n\n")
            f.write("M√âTRICAS PRINCIPAIS:\n")
            f.write("- Macro F1: M√©dia harm√¥nica de precision e recall (0-1, maior √© melhor)\n")
            f.write("- Avg Precision: √Årea sob curva precision-recall (0-1, maior √© melhor)\n")
            f.write("- Hamming Loss: Taxa de erro por classe (0-1, menor √© melhor)\n")
        
        print(f"üìñ Guia de leitura: {readme_path}")
        print(f"\n‚úÖ Relat√≥rio completo salvo em: {output_path}")
    
    def run_complete_analysis(self):
        """Executa an√°lise completa."""
        print("\n" + "="*80)
        print("üöÄ AN√ÅLISE COMPLETA MULTI-MODELO BERT")
        print("="*80)
        
        # Passo 1: Carregar dados
        if not self.load_all_results():
            return False
        
        # Passo 2: Criar tabela comparativa
        self.create_comparison_table()
        
        # Passo 3: Mostrar rankings
        self.show_rankings()
        
        # Passo 4: Analisar classes problem√°ticas
        self.analyze_problematic_classes()
        
        # Passo 5: Gerar visualiza√ß√µes (agora separadas)
        self.create_separated_plots() # Chame a nova fun√ß√£o aqui
        
        # Passo 6: Gerar relat√≥rio final
        self.generate_final_report()
        
        print("\n" + "="*80)
        print("üéâ AN√ÅLISE COMPLETA FINALIZADA!")
        print("="*80)
        print("üìÅ Verifique a pasta 'analysis_output' para os resultados")
        print("üìä Gr√°ficos salvos individualmente na pasta 'analysis_output'")
        print("üìÑ Relat√≥rio principal em 'analysis_output/final_report.txt'")
        
        return True

# ============================================================================
# FUN√á√ÉO PRINCIPAL PARA EXECU√á√ÉO
# ============================================================================

def analyze_my_bert_models(base_path: str = "."):
    """
    FUN√á√ÉO PRINCIPAL - Execute esta para fazer toda a an√°lise!
    
    Args:
        base_path: Diret√≥rio onde est√£o as pastas I001, I002, etc.
    
    Returns:
        bool: True se bem-sucedido
    """
    analyzer = SimpleMultiModelAnalyzer(base_path)
    return analyzer.run_complete_analysis()

# ============================================================================
# VERIFICA√á√ÉO E EXECU√á√ÉO AUTOM√ÅTICA
# ============================================================================

if __name__ == "__main__":
    print("üîç VERIFICANDO ARQUIVOS DE RESULTADOS...")
    
    # Verificar se existem diret√≥rios de resultados
    current_dir = Path(".")
    model_dirs = []
    
    for i in range(1, 10):
        dir_name = f"I{i:03d}"
        dir_path = current_dir / dir_name
        results_file = dir_path / "results.json"
        
        if dir_path.exists() and results_file.exists():
            model_dirs.append(dir_name)
            print(f"   ‚úÖ {dir_name} - OK")
        else:
            print(f"   ‚ùå {dir_name} - N√ÉO ENCONTRADO")
    
    print(f"\nüìä ENCONTRADOS: {len(model_dirs)}/9 modelos")
    
    if len(model_dirs) > 0:
        print("\nüöÄ INICIANDO AN√ÅLISE AUTOM√ÅTICA...")
        success = analyze_my_bert_models(".")
        
        if success:
            print("\n‚úÖ AN√ÅLISE CONCLU√çDA COM SUCESSO!")
            print("\nüìÅ RESULTADOS DISPON√çVEIS EM:")
            print("   - analysis_output/final_report.txt")
            print("   - analysis_output/comparison_complete.csv") 
            print("   - analysis_output/heatmap_model_loss.png") # Nome do arquivo alterado
            print("   - analysis_output/ranking_geral.png")
            print("   - analysis_output/f1_problematic_classes.png")
            print("   - analysis_output/effectiveness_loss_function.png")
        else:
            print("\n‚ùå ERRO NA AN√ÅLISE!")
    else:
        print("\n‚ö†Ô∏è  NENHUM MODELO ENCONTRADO!")
        print("\nPara usar este analisador:")
        print("1. Execute seus treinamentos BERT primeiro")
        print("2. Certifique-se de que existem pastas I001, I002, ..., I009")
        print("3. Cada pasta deve ter um arquivo 'results.json'")
        print("4. Execute este script novamente")
        
        print("\nüí° EXEMPLO DE EXECU√á√ÉO MANUAL:")
        print("   python analyze_results.py")
        print("   # ou")
        print("   success = analyze_my_bert_models('caminho/para/resultados')")